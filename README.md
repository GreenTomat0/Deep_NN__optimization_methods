# Deep_NN__optimization_methods

The code compares Deep Neural Network optimisation methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam.

Then exponential learning rate decay applied to a 3-layer neural network in three different optimizer modes see how each one differs.
